{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of \"Natural Language Processing with Disaster Tweets\" Competition\n",
    "\n",
    "Submission by: Atul Parida\n",
    "\n",
    "## Overview\n",
    "This notebook contains the code and analysis for the \"Natural Language Processing with Disaster Tweets\" competition on Kaggle. In this competition, the goal is to build a machine learning model that predicts whether tweets are about real disasters or not. We will explore the dataset, preprocess the text data, build and evaluate NLP models, and make predictions.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Exploration](#data-exploration)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "3. [Model Building](#model-building)\n",
    "4. [Model Evaluation](#model-evaluation)\n",
    "5. [Results and Conclusion](#results-and-conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESSENTIAL IMPORTS ###\n",
    "\n",
    "import jax.numpy as np\n",
    "import jax\n",
    "from jax import jit, vmap, grad\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, shutil\n",
    "import nltk\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA IMPORTS ###\n",
    "cwd = os.getcwd()\n",
    "test_data_path = os.path.join(cwd, \"data\", \"test.csv\")\n",
    "train_data_path = os.path.join(cwd, \"data\", \"train.csv\")\n",
    "\n",
    "test_op_path = os.path.join(cwd, \"outputs\", \"test_output.csv\")\n",
    "train_op_path = os.path.join(cwd, \"outputs\", \"train_output.csv\")\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REVIEWING FORMATS ###\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the typical parameters of each row are the keyword, location, text, and target value, with 1 being a disaster-related tweet and 0 being a non-disaster-related tweet. Each tweet text can contain additional elements such as emojis, hashtags, URLs, and HTML tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data:  7613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length of training data: \", len(train_data))\n",
    "\n",
    "train_data.isnull().sum() # Find the number of null values to determine the correct approach for data cleaning and imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target.value_counts() # Find the number of disaster and non-disaster tweets in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of testing data:  3263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length of testing data: \", len(test_data))\n",
    "\n",
    "test_data.isnull().sum() # Find the number of null values to determine the correct approach for data cleaning and imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique locations vs total locations in training dataset: 3341 unique in 5080 total\n",
      "Unique locations vs total locations in testing dataset: 1602 unique in 2158 total\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique locations vs total locations in training dataset: {train_data[\"location\"].nunique()} unique in {len(train_data) - train_data.location.isnull().sum()} total\")\n",
    "print(f\"Unique locations vs total locations in testing dataset: {test_data[\"location\"].nunique()} unique in {len(test_data) - test_data.location.isnull().sum()} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique keywords vs total keywords in training dataset: 221 unique in 7552 total\n",
      "Unique keywords vs total keywords in testing dataset: 221 unique in 3237 total\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique keywords vs total keywords in training dataset: {train_data[\"keyword\"].nunique()} unique in {len(train_data) - train_data.keyword.isnull().sum()} total\")\n",
    "print(f\"Unique keywords vs total keywords in testing dataset: {test_data[\"keyword\"].nunique()} unique in {len(test_data) - test_data.keyword.isnull().sum()} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common keywords in training and testing datasets: 221\n",
      "Training keywords: 221\n",
      "Testing keywords: 221\n"
     ]
    }
   ],
   "source": [
    "train_kw_dict = { }\n",
    "test_kw_dict = { }\n",
    "for kw in train_data[\"keyword\"]:\n",
    "    if str(kw) == 'nan':\n",
    "        continue\n",
    "    elif kw in train_kw_dict.keys():\n",
    "        train_kw_dict[str(kw)] += 1\n",
    "    else:\n",
    "        train_kw_dict[str(kw)] = 1\n",
    "\n",
    "for kw in test_data[\"keyword\"]:\n",
    "    if str(kw) == 'nan':\n",
    "        continue\n",
    "    elif kw in test_kw_dict.keys():\n",
    "        test_kw_dict[str(kw)] += 1\n",
    "    else:\n",
    "        test_kw_dict[str(kw)] = 1\n",
    "\n",
    "common_kw = []\n",
    "for kw in test_kw_dict.keys():\n",
    "    if kw in train_kw_dict.keys():\n",
    "        common_kw.append(kw)\n",
    "\n",
    "print(f\"Number of common keywords in training and testing datasets: { len(common_kw) }\")\n",
    "print(f\"Training keywords: {len(train_kw_dict.keys())}\")\n",
    "print(f\"Testing keywords: {len(test_kw_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, both the training and testing datasets have roughly 33% of tweets which do not contain the location, and roughly 0.8 to 1% of tweets with no keyword provided. This suggests that the training and testing datasets were both taken from the same sample dataset of disaster-related tweets. \n",
    "\n",
    "The reason for a lack of locations is because they are based on user inputs, and not automatically appended to the tweet. Additionally, not all locations provided by the user may correspond to an actual location, hence having a high number of unique values which may skew the model. Due to this, ```location``` won't be considered as a feature.\n",
    "\n",
    "The total number of keywords appears to be the same, and the keywords themselves appear to be the same between both the training and testing datasets, which means that ```keywords``` can be used as a feature. This also suggests the sampling of the training and testing datasets from the same parent dataset.\n",
    "\n",
    "From the features provided and their general characteristics, we can also define some metafeatures which can assist in increasing our model's accuracy. This can include:\n",
    "- **Word count:** number of total words in the text\n",
    "- **Unique word count:** number of unique words in the text\n",
    "- **Stop word count:** number of stop words in the text\n",
    "- **Char count:** number of characters used in the text\n",
    "- **Mean word length:** mean length of words used in the text\n",
    "- **Slang word count:** number of slang terms used in the text.\n",
    "\n",
    "More general metafeatures are suggested as they are the most generalisable to tweets, irrespective of whether they include URLs or emojis or other characteristics. It makes the analysis more objective.\n",
    "\n",
    "Data that can be cleaned up from the tweets include the following:\n",
    "- **Hashtags:**\n",
    "- **Mentions:**\n",
    "- **URLs:**\n",
    "- **Punctuation:**\n",
    "- **Stop words:**\n",
    "- **Special characters and emojis:**\n",
    "- **Slang words:**\n",
    "\n",
    "Additional cleaning steps that can be undertaken would include lowercase transformation, duplicate space removal, spell-correction, and lemmatization of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING PREPROCESSING METHODS ###\n",
    "def include_word_count(input_df): # Addition of word_count metafeature\n",
    "    pass\n",
    "\n",
    "def include_unique_word_count(input_df): # Addition of unique_word_count metafeature\n",
    "    pass\n",
    "\n",
    "def include_stop_word_count(input_df): # Addition of stop_word_count metafeature\n",
    "    pass\n",
    "\n",
    "def include_char_count(input_df): # Addition of char_count metafeaure\n",
    "    pass\n",
    "\n",
    "def include_mean_word_length(input_df): # Addition of mean_word_length metafeature\n",
    "    pass\n",
    "\n",
    "def keyword_imputation(input_df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING METHODS FOR REMOVAL ###\n",
    "def remove_stop_words(input_df):\n",
    "    pass\n",
    "\n",
    "def remove_URLs(input_df):\n",
    "    pass\n",
    "\n",
    "def remove_punctuation(input_df):\n",
    "    pass\n",
    "\n",
    "def remove_mentions(input_df):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING ESSENTIAL CLASSES AND FUNCTIONS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANALYZING HYPERPARAMETER VALUES AND BEHAVIOUR ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMAL MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMAL RESULTS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump to top](#analysis-of-natural-language-processing-with-disaster-tweets-competition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
